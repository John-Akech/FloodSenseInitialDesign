{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-title",
   "metadata": {},
   "source": [
    "# FloodSense: Machine Learning-Based Flood Prediction System\n",
    "## Comprehensive ML Pipeline for SEN12-FLOOD Dataset Analysis\n",
    "\n",
    "**Author:** FloodSense Research Team  \n",
    "**Date:** December 2024  \n",
    "**Dataset:** SEN12-FLOOD Satellite Imagery Metadata  \n",
    "**Objective:** Develop and evaluate ML models for flood prediction in South Sudan\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "This notebook implements a systematic machine learning pipeline with four progressive experiments:\n",
    "\n",
    "1. **Experiment 1:** Baseline Model Establishment\n",
    "2. **Experiment 2:** Hyperparameter Optimization & Class Imbalance Handling\n",
    "3. **Experiment 3:** Advanced Ensemble Methods (XGBoost)\n",
    "4. **Experiment 4:** Feature Engineering Enhancement\n",
    "\n",
    "### Key Research Questions\n",
    "- Can satellite metadata effectively predict flood events?\n",
    "- Which ML algorithms perform best for this task?\n",
    "- How does class imbalance handling impact performance?\n",
    "- What feature engineering strategies are most effective?\n",
    "\n",
    "### Expected Outcomes\n",
    "- Achieve >90% accuracy across all performance metrics\n",
    "- Identify optimal algorithm and hyperparameters\n",
    "- Create reproducible ML pipeline for flood prediction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "### 1.1 Install Required Packages\n",
    "\n",
    "We use specific package versions to ensure reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "package-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear pip cache and install dependencies with fixed versions\n",
    "!pip cache purge\n",
    "!pip install pandas==2.2.2 numpy==1.26.4 scikit-learn==1.4.2 matplotlib==3.7.1 seaborn==0.13.0 imbalanced-learn==0.10.1 xgboost==1.7.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-section",
   "metadata": {},
   "source": [
    "### 1.2 Import Libraries and Set Configuration\n",
    "\n",
    "Import all necessary libraries and configure the environment for reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries for data processing and machine learning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "# Import scikit-learn components\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    precision_recall_curve, auc, roc_curve\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import XGBoost for advanced ensemble methods\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Import joblib for model persistence\n",
    "import joblib\n",
    "\n",
    "# Configure environment for reproducible results\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)  # CRITICAL: Fixed seed for reproducibility\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print('âœ… Dependencies imported successfully')\n",
    "print('âœ… Random seed set to 42 for reproducibility')\n",
    "print('âœ… Environment configured for systematic experimentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-section",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration\n",
    "\n",
    "### 2.1 Dataset Overview\n",
    "\n",
    "**Dataset:** SEN12-FLOOD Satellite Imagery Metadata  \n",
    "**Geographic Focus:** South Sudan regions (Jonglei, Unity, Upper Nile)  \n",
    "**Purpose:** Extract meaningful features from satellite metadata for flood prediction\n",
    "\n",
    "### 2.2 Data Loading Strategy\n",
    "\n",
    "We load the SEN12-FLOOD dataset and perform initial exploration to understand:\n",
    "- Data structure and format\n",
    "- Available features and metadata\n",
    "- Data quality and completeness\n",
    "- Class distribution (flood vs non-flood events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ðŸ“Š Loading SEN12-FLOOD Dataset')\n",
    "print('Dataset: Sentinel-2 satellite imagery metadata for flood detection')\n",
    "print('Geographic Focus: South Sudan regions (Jonglei, Unity, Upper Nile)')\n",
    "print('=' * 80)\n",
    "\n",
    "data_dir = r'D:\\FloodSenseSystem\\SEN12FLOODDATA'\n",
    "s2_json_path = os.path.join(data_dir, 'S2list.json')\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    with open(s2_json_path, 'r') as f:\n",
    "        s2_data = json.load(f)\n",
    "    df = pd.DataFrame(s2_data)\n",
    "    print(f\"Dataset loaded successfully: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {s2_json_path} not found\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-section",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing & Feature Engineering\n",
    "\n",
    "### 3.1 Feature Engineering Strategy\n",
    "\n",
    "**Objective:** Extract meaningful features from satellite metadata that can predict flood events\n",
    "\n",
    "**Feature Categories:**\n",
    "- **Temporal Features:** Date-based patterns (month, day, seasonality)\n",
    "- **Spatial Context:** Geographic and scene-based information\n",
    "- **Data Quality:** Coverage, completeness, and reliability indicators\n",
    "- **Metadata Features:** File properties and observation characteristics\n",
    "\n",
    "### 3.2 Missing Value Handling\n",
    "\n",
    "We implement robust strategies for handling missing or invalid data:\n",
    "- Skip incomplete observations\n",
    "- Use latest available labels for classification\n",
    "- Extract features only from valid scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-initialization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature extraction variables\n",
    "print('ðŸ”§ ENHANCED FEATURE EXTRACTION')\n",
    "print('Strategy: Extract meaningful features from satellite metadata')\n",
    "print('Focus: Temporal patterns, spatial context, data quality indicators')\n",
    "print('=' * 80)\n",
    "\n",
    "target_column = 'flood'\n",
    "X = []\n",
    "y = []\n",
    "valid_indices = []\n",
    "scene_ids = [col for col in df.columns if col not in ['count', 'folder', 'geo']]\n",
    "\n",
    "print(f'Total scenes available: {len(scene_ids)}')\n",
    "print(f'Dataset shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-exploration",
   "metadata": {},
   "source": [
    "### 3.3 Data Structure Analysis\n",
    "\n",
    "Before feature extraction, we need to understand the data structure and identify available metadata fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-data-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyzing data structure and extracting meaningful features...\")\n",
    "\n",
    "# First, let's understand what data we actually have\n",
    "print(\"Sample data exploration:\")\n",
    "sample_count = 0\n",
    "for idx in df.index:\n",
    "    if idx in ['count', 'folder', 'geo']:\n",
    "        continue\n",
    "    for scene_id in scene_ids[:3]:  # Check first 3 scenes\n",
    "        cell_value = df.loc[idx, scene_id]\n",
    "        if isinstance(cell_value, dict):\n",
    "            print(f\"Observation {idx}, Scene {scene_id}:\")\n",
    "            for key, value in cell_value.items():\n",
    "                print(f\"  {key}: {value} (type: {type(value).__name__})\")\n",
    "            sample_count += 1\n",
    "            if sample_count >= 3:\n",
    "                break\n",
    "    if sample_count >= 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-extraction-process",
   "metadata": {},
   "source": [
    "### 3.4 Feature Extraction Implementation\n",
    "\n",
    "**Feature Engineering Strategy:**\n",
    "\n",
    "1. **Temporal Features:** Extract date-based patterns (month, day, seasonality)\n",
    "2. **Spatial Context:** Scene-based geographic information\n",
    "3. **Data Quality:** Coverage and completeness indicators\n",
    "4. **Metadata Features:** File properties and observation characteristics\n",
    "\n",
    "**Missing Value Strategy:** Use latest available labels and skip incomplete observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract meaningful features\n",
    "print(\"\\nExtracting features from all scenes...\")\n",
    "\n",
    "for idx in df.index:\n",
    "    if idx in ['count', 'folder', 'geo']:\n",
    "        continue\n",
    "    \n",
    "    # Find the latest scene for the label\n",
    "    latest_date = None\n",
    "    latest_label = None\n",
    "    for scene_id in scene_ids:\n",
    "        cell_value = df.loc[idx, scene_id]\n",
    "        if not isinstance(cell_value, dict) or 'FLOODING' not in cell_value or 'date' not in cell_value:\n",
    "            continue\n",
    "        date = pd.to_datetime(cell_value.get('date', '1900-01-01'))\n",
    "        if latest_date is None or date > latest_date:\n",
    "            latest_date = date\n",
    "            latest_label = 1 if cell_value['FLOODING'] else 0\n",
    "    \n",
    "    if latest_label is None:\n",
    "        continue\n",
    "    \n",
    "    # Extract features for all valid scenes\n",
    "    for scene_id in scene_ids:\n",
    "        cell_value = df.loc[idx, scene_id]\n",
    "        if not isinstance(cell_value, dict) or 'FLOODING' not in cell_value:\n",
    "            continue\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        # 1. Date-based features (temporal patterns)\n",
    "        if 'date' in cell_value:\n",
    "            try:\n",
    "                date = pd.to_datetime(cell_value['date'])\n",
    "                features.extend([\n",
    "                    date.month,\n",
    "                    date.day,\n",
    "                    date.dayofweek,\n",
    "                    date.dayofyear,\n",
    "                    date.quarter,\n",
    "                    (date - pd.Timestamp('2000-01-01')).days\n",
    "                ])\n",
    "            except:\n",
    "                features.extend([0, 0, 0, 0, 0, 0])\n",
    "        else:\n",
    "            features.extend([0, 0, 0, 0, 0, 0])\n",
    "        \n",
    "        # 2. Scene-based features\n",
    "        scene_numeric = hash(scene_id) % 10000\n",
    "        features.append(scene_numeric)\n",
    "        \n",
    "        # 3. Data quality features\n",
    "        data_coverage = len([k for k, v in cell_value.items() if v is not None])\n",
    "        features.append(data_coverage)\n",
    "        \n",
    "        # 4. Metadata features\n",
    "        filename = cell_value.get('filename', '')\n",
    "        features.extend([\n",
    "            len(filename),\n",
    "            hash(filename) % 1000,\n",
    "            len(X)  # observation index\n",
    "        ])\n",
    "        \n",
    "        X.append(features)\n",
    "        y.append(latest_label)\n",
    "        valid_indices.append(f\"{idx}_{scene_id}\")\n",
    "\n",
    "print(f'Feature extraction completed: {len(X)} samples with {len(X[0]) if X else 0} features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-summary",
   "metadata": {},
   "source": [
    "### 3.5 Dataset Summary and Class Distribution\n",
    "\n",
    "After feature extraction, we analyze the final dataset characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays for ML processing\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f'Dataset created successfully: {X.shape}')\n",
    "print(f'Features: {X.shape[1]}')\n",
    "print(f'Samples: {X.shape[0]}')\n",
    "\n",
    "# Class distribution analysis\n",
    "flood_count = np.sum(y == 1)\n",
    "no_flood_count = np.sum(y == 0)\n",
    "flood_ratio = flood_count / len(y)\n",
    "\n",
    "print(f'Class Distribution:')\n",
    "print(f'Flood: {flood_count}, No Flood: {no_flood_count}')\n",
    "print(f'Flood ratio: {flood_ratio:.3f}')\n",
    "\n",
    "# Feature names for reference\n",
    "feature_names = [\n",
    "    'month', 'day', 'day_of_week', 'day_of_year', 'quarter',\n",
    "    'days_since_reference', 'scene_id_numeric', 'data_coverage',\n",
    "    'filename_length', 'filename_hash', 'observation_index'\n",
    "]\n",
    "\n",
    "print(f'Feature names: {feature_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preprocessing",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing and Normalization\n",
    "\n",
    "### 4.1 Train-Test Split Strategy\n",
    "\n",
    "We use stratified sampling to maintain class distribution across train/test sets:\n",
    "- **Training Set:** 80% of data for model training\n",
    "- **Test Set:** 20% of data for final evaluation\n",
    "- **Stratification:** Ensures balanced flood/non-flood distribution\n",
    "\n",
    "### 4.2 Feature Scaling\n",
    "\n",
    "StandardScaler normalization ensures all features contribute equally to model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-test-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f'Training set: {X_train.shape[0]} samples')\n",
    "print(f'Test set: {X_test.shape[0]} samples')\n",
    "print(f'Training flood ratio: {np.mean(y_train):.3f}')\n",
    "print(f'Test flood ratio: {np.mean(y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-scaling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling for consistent model performance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('âœ… Feature scaling completed')\n",
    "print(f'Scaled training features shape: {X_train_scaled.shape}')\n",
    "print(f'Feature means: {np.mean(X_train_scaled, axis=0)[:5]}')  # Show first 5\n",
    "print(f'Feature stds: {np.std(X_train_scaled, axis=0)[:5]}')   # Show first 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment-1",
   "metadata": {},
   "source": [
    "## 5. Experiment 1: Baseline Model Establishment\n",
    "\n",
    "### 5.1 Experiment Objective\n",
    "\n",
    "**Goal:** Establish baseline performance with standard ML algorithms\n",
    "**Models:** Logistic Regression, Random Forest, XGBoost (default parameters)\n",
    "**Evaluation:** Accuracy, Precision, Recall, F1-Score\n",
    "\n",
    "### 5.2 Model Training and Evaluation\n",
    "\n",
    "We train multiple models to identify the best baseline approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize baseline models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {'accuracy': accuracy, 'f1': f1}\n",
    "    \n",
    "    print(f'{name}: Accuracy={accuracy:.4f}, F1={f1:.4f}')\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(results.keys(), key=lambda k: results[k]['f1'])\n",
    "best_f1 = results[best_model_name]['f1']\n",
    "\n",
    "print(f'\\nBest Model: {best_model_name}')\n",
    "print(f'Best F1 Score: {best_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-deployment",
   "metadata": {},
   "source": [
    "## 6. Model Deployment and Persistence\n",
    "\n",
    "### 6.1 Save Best Model\n",
    "\n",
    "We save the best performing model and preprocessing components for production deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Get the best model\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Save model artifacts\n",
    "joblib.dump(best_model, 'models/flood_prediction_model.pkl')\n",
    "joblib.dump(scaler, 'models/feature_scaler.pkl')\n",
    "\n",
    "# Save model metadata\n",
    "model_info = {\n",
    "    'model_type': best_model_name,\n",
    "    'accuracy': results[best_model_name]['accuracy'],\n",
    "    'f1_score': results[best_model_name]['f1'],\n",
    "    'features': len(feature_names),\n",
    "    'training_samples': len(X_train),\n",
    "    'feature_names': feature_names\n",
    "}\n",
    "\n",
    "joblib.dump(model_info, 'models/model_info.pkl')\n",
    "joblib.dump(feature_names, 'models/feature_names.pkl')\n",
    "\n",
    "print('âœ… Model artifacts saved successfully')\n",
    "print(f'Final F1 Score: {best_f1:.4f}')\n",
    "print(f'Model files saved to: models/ directory')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}